sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
simtbh()
tpvals <- function(x,y) {
labels <- unique(x)
combinations <- combn(labels, 2)
p_val <- c()
df <- data.frame(x,y)
for (i in 1:ncol(combinations)) {
vector1 <- df[which(df$x == combinations[1,i]),'y']
vector2 <- df[which(df$x == combinations[2,i]),'y']
p_val <- cbind(p_val, t.test(vector1, vector2, paired=F, var.equal=F)$p.value)
}
return (p_val)
}
simxy = function( n=rep(5,20), seed = 20141021 ){
# Simulates one null data set with 5 obs in each of 20 groups
set.seed(seed)
x = rep(1:length(n),n)
N = sum(n)
y = rnorm(N)
data.frame(x=x,y=y)
}
simtbh = function( nrep=100, n=rep(5,20), seed=20141021 ){
# Simulates nrep null data sets with (default) 5 obs in each of 20 groups
# Use smaller nrep while testing!
k = length(n)
ans = matrix(0,nrep,choose(k,2))
for( i in 1:nrep ){
thesim = simxy(seed=seed+i)
ans[i,] = sort(tpvals(thesim$x,thesim$y))
}
ans
}
bh = function(v,alpha=.1){
m <- length (v)
sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
simtbh()
class(ans)
dim(ans)
length(which(ans < 0.1)) / (nrow(ans) * ncol(ans))
length(which(ans < 0.1))
dim(ans)
apply(ans, 1, < 0.1)
apply(ans, 1, function(x) length(which(x) < 0.1))
ans[1,]
head(ans)
tpvals <- function(x,y) {
labels <- unique(x)
combinations <- combn(labels, 2)
p_val <- c()
df <- data.frame(x,y)
for (i in 1:ncol(combinations)) {
vector1 <- df[which(df$x == combinations[1,i]),'y']
vector2 <- df[which(df$x == combinations[2,i]),'y']
p_val <- cbind(p_val, t.test(vector1, vector2, paired=F, var.equal=F)$p.value)
}
return (p_val)
}
simxy = function( n=rep(5,20), seed = 20141021 ){
# Simulates one null data set with 5 obs in each of 20 groups
set.seed(seed)
x = rep(1:length(n),n)
N = sum(n)
y = rnorm(N)
data.frame(x=x,y=y)
}
simtbh = function( nrep=100, n=rep(5,20), seed=20141021 ){
# Simulates nrep null data sets with (default) 5 obs in each of 20 groups
# Use smaller nrep while testing!
k = length(n)
ans = matrix(0,nrep,choose(k,2))
for( i in 1:nrep ){
thesim = simxy(seed=seed+i)
ans[i,] = sort(tpvals(thesim$x,thesim$y))
}
return (ans)
}
bh = function(v,alpha=.1){
m <- length (v)
sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
simtbh()
dim(ans)
head(ans)
tpvals <- function(x,y) {
labels <- unique(x)
combinations <- combn(labels, 2)
p_val <- c()
df <- data.frame(x,y)
for (i in 1:ncol(combinations)) {
vector1 <- df[which(df$x == combinations[1,i]),'y']
vector2 <- df[which(df$x == combinations[2,i]),'y']
p_val <- cbind(p_val, t.test(vector1, vector2, paired=F, var.equal=F)$p.value)
}
return (p_val)
}
simxy = function( n=rep(5,20), seed = 20141021 ){
# Simulates one null data set with 5 obs in each of 20 groups
set.seed(seed)
x = rep(1:length(n),n)
N = sum(n)
y = rnorm(N)
data.frame(x=x,y=y)
}
simtbh = function( nrep=100, n=rep(5,20), seed=20141021 ){
# Simulates nrep null data sets with (default) 5 obs in each of 20 groups
# Use smaller nrep while testing!
k = length(n)
ans = matrix(0,nrep,choose(k,2))
for( i in 1:nrep ){
thesim = simxy(seed=seed+i)
ans[i,] = sort(tpvals(thesim$x,thesim$y))
}
ans
}
bh = function(v,alpha=.1){
m <- length (v)
sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
pval_matrix <- simtbh()
dim(pval_matrix)
length(which(pval_matrix < 0.1)) / (nrow(pval_matrix) * ncol(pval_matrix))
apply(pval_matrix, 1, function(x) length(which(x < 0.1)))
apply(pval_matrix, 1, function(x) length(which(x < 0.1) / ncol(pval_matrix)))
apply(pval_matrix, 1, function(x) length(which(x < 0.1) / ncol(pval_matrix)))
apply(pval_matrix, 1, function(x) length(which(x < 0.1) / length(x)))
apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) / 190})
apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) / length(x)})
apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0})
sum(apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0}))
sum(apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0})) / nrow(pval_matrix)
pval_matrix[25,]
pval_matrix[99,]
1 - sum(apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0})) / nrow(pval_matrix)
apply(pval_matrix, 1, bh)
apply(pval_matrix, 1, bh)
install.packages('ggplot12')
install.packages('ggplot2')
tpvals <- function(x,y) {
labels <- unique(x)
combinations <- combn(labels, 2)
p_val <- c()
df <- data.frame(x,y)
for (i in 1:ncol(combinations)) {
vector1 <- df[which(df$x == combinations[1,i]),'y']
vector2 <- df[which(df$x == combinations[2,i]),'y']
p_val <- cbind(p_val, t.test(vector1, vector2, paired=F, var.equal=F)$p.value)
}
return (p_val)
}
simxy = function( n=rep(5,20), seed = 20141021 ){
# Simulates one null data set with 5 obs in each of 20 groups
set.seed(seed)
x = rep(1:length(n),n)
N = sum(n)
y = rnorm(N)
data.frame(x=x,y=y)
}
simtbh = function( nrep=100, n=rep(5,20), seed=20141021 ){
# Simulates nrep null data sets with (default) 5 obs in each of 20 groups
# Use smaller nrep while testing!
k = length(n)
ans = matrix(0,nrep,choose(k,2))
for( i in 1:nrep ){
thesim = simxy(seed=seed+i)
ans[i,] = sort(tpvals(thesim$x,thesim$y))
}
ans
}
bh = function(v,alpha=.1){
m <- length (v)
sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
# g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
pval_matrix <- simtbh()
1 - sum(apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0})) / nrow(pval_matrix)    # Ask at Office Hours
apply(pval_matrix, 1, bh)
tpvals <- function(x,y) {
labels <- unique(x)
combinations <- combn(labels, 2)
p_val <- c()
df <- data.frame(x,y)
for (i in 1:ncol(combinations)) {
vector1 <- df[which(df$x == combinations[1,i]),'y']
vector2 <- df[which(df$x == combinations[2,i]),'y']
p_val <- cbind(p_val, t.test(vector1, vector2, paired=F, var.equal=F)$p.value)
}
return (p_val)
}
simxy = function( n=rep(5,20), seed = 20141021 ){
# Simulates one null data set with 5 obs in each of 20 groups
set.seed(seed)
x = rep(1:length(n),n)
N = sum(n)
y = rnorm(N)
data.frame(x=x,y=y)
}
simtbh = function( nrep=100, n=rep(5,20), seed=20141021 ){
# Simulates nrep null data sets with (default) 5 obs in each of 20 groups
# Use smaller nrep while testing!
k = length(n)
ans = matrix(0,nrep,choose(k,2))
for( i in 1:nrep ){
thesim = simxy(seed=seed+i)
ans[i,] = sort(tpvals(thesim$x,thesim$y))
}
ans
}
bh = function(v,alpha=.1){
m <- length (v)
sorted.v <- sort(v)
index <- 1:m
Li <- index*alpha / m
reject.H0 <- 0
df <- data.frame(index,Li,sorted.v)
R <- max(which(sorted.v <= Li),0)
# g <- ggplot(df,aes(x = index)) + labs(x ="Index", y="p-values") +geom_line(aes(y=Li), linetype = 2,colour="red") + geom_line(aes(y=sorted.v),colour="blue") +geom_vline(xintercept = R,colour = "red",linetype=4)
T <- max(sorted.v[R],0)
reject.H0 <- v <= T
# print(g)
sum(reject.H0)
}
# bh = function(v,alpha=.1){
# # Write the BH function
# # v is a vector of p values that should be sorted in increasing order
#
# stop("You get to write the Benjamini-Hochberg function.")
# # remove this stop command after you've coded the function
#
# v = sort(v) # (just in case)
#
#
# }
pval_matrix <- simtbh()
1 - sum(apply(pval_matrix, 1, function(x) {length(which(x < 0.1)) > 0})) / nrow(pval_matrix)    # Ask at Office Hours
apply(pval_matrix, 1, bh)
library(randomForest)
bag.model <- randomForest(yTrain~.,data=subsetDF,mtry=10,importance=TRUE)
bag.model
preds.bag <- predict(bag.model,newdata=xTest)
sqrt(mean((preds.bag-yTest)^2))
install.packages('randomForrest')
install.packages('randomForest')
install.packages("randomForest")
library(randomForest)
bag.model <- randomForest(yTrain~.,data=subsetDF,mtry=10,importance=TRUE)
subsetDF <- as.data.frame(cbind(xTrain, yTrain))
bag.model <- randomForest(yTrain~.,data=subsetDF,mtry=10,importance=TRUE)
# Clear all
rm(list=ls())
# Load libraries
library(glmnet)
library(leaps)
library(MASS)
library(ISLR)
library(pls)
library(randomForest)
#---------------------------------------------------------------------
# Set working directory
setwd('~/Documents/Stanford/CS229/CS229-Project/Merge/')
# setwd("/Users/alysonkane/desktop/cs229/census_aly")
# Import datasets
crimes <- read.csv('Aggregated_Crimes.csv', header=T, stringsAsFactors=F)
census <- read.csv('census_total.csv', header=T, stringsAsFactors=F)
# Merge datasets
merged <- merge(crimes, census, by.x=c('City', 'Tract'), by.y=c('city', 'census_tract'), all.x=T)
names(merged) <- c('City', 'Tract', 'x', 'Frequency', 'y', 'Id', 'TotalPop', 'PopUnder5years',
'Pop5to9years', 'Pop10to14years', 'Pop15to19years', 'Pop20to24years',
'Pop25to29years', 'Pop30to34years', 'Pop35to39years', 'Pop40to44years',
'Pop45to49years', 'Pop50to54years', 'Pop55to59years', 'Pop60to64years',
'Pop65to69years', 'Pop70to74years', 'Pop75to79years', 'Pop80to84years',
'Pop85yearsandover', 'MedianAge', 'Pop16yearsandover', 'Pop18yearsandover',
'Pop21yearsandover', 'Pop62yearsandover', 'Pop65yearsandover', 'MalePop',
'FemalePop', 'RaceWhite', 'RaceBlack', 'RaceNative', 'RaceAsian',
'RacePacificIslander', 'RaceOther', 'EthnHispanic', 'FamilyhouseholdsAll',
'FamilyhouseholdChildunder18', 'FamilyhouseholdsHusbandwife',
'FamilyhouseholdsMalehouseholder', 'FamilyhouseholdsFemalehouseholder',
'NonfamilyhouseholdsAll', 'Nonfamilyhouseholdslivingalone',
'NonfamilyhouseholdslivingaloneMale', 'NonfamilyhouseholdslivingaloneFemale',
'TotalHouseholdsUnder18years', 'TotalHouseholds65yearsover', 'AvgHHsize',
'TotalHousingUnits', 'HousingVacantunits', 'HousingVacantunitsseasonal',
'HousingOwneroccupied')
merged <- merged[,-which(names(merged) %in% c('Tract','x','y','Id'))]
class(merged$City) <- as.factor(merged$City)
merged[,-1] <- apply(merged[,-1], 2, as.numeric)
merged <- merged[merged$TotalPop != 0,]
merged$Response <- merged$Frequency / merged$TotalPop
merged <- merged[,-which(names(merged) == 'Frequency')]
merged$logResponse <- log(merged$Response)
#---------------------------------------------------------------------
## Data Visualization
setwd('~/Documents/Stanford/CS229/CS229-Project/Visualizations/')
jpeg('ResponseHistogram.jpg')
hist(merged$Response,xlim=c(0,1),breaks=10000, xlab='Nominal Rate', main='Crime Rate')
dev.off()
jpeg('LogResponseHistogram.jpg')
hist(merged$logResponse,breaks=100, xlab='Log Rate', main='Log Crime Rate')
dev.off()
#---------------------------------------------------------------------
## Modeling/Prediction - Formatting data
# Center and scale data
cols_to_center <- which(!names(merged) %in% c('City','Response','logResponse'))
merged[,cols_to_center] <- scale(merged[,cols_to_center], center=T, scale=T)
## create dataset with Response variable and logResponse variable
mergedLog <- merged[,-which(names(merged) == 'Response')]
mergedResponse <- merged[,-which(names(merged) == 'logResponse')]
# Test/train split
trainIndex <- sample(1:nrow(merged), 0.7*nrow(merged))
train <- mergedLog[trainIndex, ]
test <- mergedLog[-trainIndex, ]
trainR <- mergedResponse[trainIndex, ]
testR <- mergedResponse[-trainIndex, ]
#---------------------------------------------------------------------
## SUPERVISED
## Linear models: OLS, Ridge, Lasso, Elastic Net
## (1) OLS
lm.model <- lm(logResponse ~ ., data=train, na.action = na.omit)
# Calculate RMSE
lm.predict <- predict(lm.model, newdata = test)
lm.mse <- mean(na.omit(test$logResponse - lm.predict)^2)
lm.rmse <- sqrt(lm.mse)
sprintf("Baseline prediction error is: %f",lm.rmse)
# Diagnostic plots
# plot(lm.model)
## (2) Poisson GLM to Response
glm.model <- glm(Response ~ ., data = trainR, family=poisson, na.action = na.omit)
glm.predict <- predict(glm.model, newdata = testR)
# Calculate RMSE
glm.mse <- mean(na.omit(testR$Response - glm.predict)^2)
glm.rmse <- sqrt(glm.mse)
sprintf("Poisson prediction error is: %f",glm.rmse)
# note: RMSE is much higher for poisson GLM.  Because of outliers (max is ~400%), model is predicting
# negative numbers for smaller percentages.  This doesn't make sense, so we should continue to use log(Response).
## (3) Ridge, Lasso, Elastic Net
# Format Training Data
design <- model.matrix(~., mergedLog)
sampleInd <- sample(1:nrow(design), 0.7*nrow(design))
# Remove response and intercept variables
Design <- design[,-which(colnames(design) %in% c('(Intercept)', 'logResponse'))]
Response <- design[,which(colnames(design) == 'logResponse')]
xTrain <- Design[sampleInd,]
xTest <- Design[-sampleInd,]
yTrain <- Response[sampleInd]
yTest <- Response[-sampleInd]
# Run penalized models
lm.ridge <- glmnet(xTrain, yTrain, alpha = 0)
lm.lasso <- glmnet(xTrain, yTrain, alpha = 1)
lm.elasticnet <- glmnet(xTrain, yTrain, alpha = .5)
# CV to tune lambda
set.seed(10)
cv.ridge <- cv.glmnet(xTrain,yTrain,alpha=0)
cv.lasso <- cv.glmnet(xTrain,yTrain,alpha=1)
cv.elasticnet <- cv.glmnet(xTrain,yTrain,alpha=.5)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.lasso <- cv.lasso$lambda.min
bestlam.elasticnet <- cv.elasticnet$lambda.min
# calculate RMSE
ridge.pred <- predict(lm.ridge, newx=xTest, s=bestlam.ridge)
lasso.pred <- predict(lm.lasso, newx=xTest, s=bestlam.lasso)
mseRidge <- mean((ridge.pred - yTest)^2)
mseLasso <- mean((lasso.pred - yTest)^2)
sprintf("Ridge prediction error is: %f", sqrt(mseRidge))
sprintf("Lasso prediction error is: %f", sqrt(mseLasso))
## Tune Alpha for Elastic Net
for (i in 1:9) {
assign(paste("fit", i, sep=""), cv.glmnet(xTrain, yTrain, type.measure="mse",
alpha=i/10, family="gaussian"))
}
yhat1 <- predict(fit1, s=fit1$lambda.1se, newx=xTest)
yhat2 <- predict(fit2, s=fit2$lambda.1se, newx=xTest)
yhat3 <- predict(fit3, s=fit3$lambda.1se, newx=xTest)
yhat4 <- predict(fit4, s=fit4$lambda.1se, newx=xTest)
yhat5 <- predict(fit5, s=fit5$lambda.1se, newx=xTest)
yhat6 <- predict(fit6, s=fit6$lambda.1se, newx=xTest)
yhat7 <- predict(fit7, s=fit7$lambda.1se, newx=xTest)
yhat8 <- predict(fit8, s=fit8$lambda.1se, newx=xTest)
yhat9 <- predict(fit9, s=fit9$lambda.1se, newx=xTest)
mse1 <- mean((yTest - yhat1)^2)
mse2 <- mean((yTest - yhat2)^2)
mse3 <- mean((yTest - yhat3)^2)
mse4 <- mean((yTest - yhat4)^2)
mse5 <- mean((yTest - yhat5)^2)
mse6 <- mean((yTest - yhat6)^2)
mse7 <- mean((yTest - yhat7)^2)
mse8 <- mean((yTest - yhat8)^2)
mse9 <- mean((yTest - yhat9)^2)
minRMSE <- min(c(sqrt(mse1),sqrt(mse2),sqrt(mse3),sqrt(mse4),sqrt(mse5),sqrt(mse6),sqrt(mse7),sqrt(mse8),sqrt(mse9)))
alpha <- which.min(c(sqrt(mse1),sqrt(mse2),sqrt(mse3),sqrt(mse4),sqrt(mse5),sqrt(mse6),sqrt(mse7),sqrt(mse8),sqrt(mse9)))
# sprintf("The min RMSE for the elastic net is: %f", minRMSE)
# sprintf("Corresponding Alpha value is: %f", alpha/10)
lm.elasticnet <- glmnet(xTrain, yTrain, alpha = alpha/10)
net.pred <- predict(lm.elasticnet, newx=xTest, s=bestlam.elasticnet)
mseNet <- mean((net.pred - yTest)^2)
sprintf("Elastic Net prediction error is: %f", sqrt(mseNet))
subsetDF <- as.data.frame(cbind(xTrain, yTrain))
set.seed(1)
pcr.model <- pcr(logResponse ~ ., data=train, scale=TRUE, validation="CV")
validationplot(pcr,val.type="MSEP")
pcr.pred <- predict(pcr,test,ncomp=5)
mse.pcr <- (pcr.pred-test$logResponse)^2
rmse.pcr <- sqrt(mean(temp[!is.na(mse.pcr)]))
bag.model <- randomForest(yTrain~.,data=subsetDF,mtry=10,importance=TRUE)
preds.bag <- predict(bag.model,newdata=xTest)
sqrt(mean((preds.bag-yTest)^2))
importance(bag.model)
varImpPlot(bag.model, main = 'Variable Importance')
